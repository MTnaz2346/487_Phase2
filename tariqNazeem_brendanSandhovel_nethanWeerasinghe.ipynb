{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5f75e81-ecdb-465a-9008-9cf9bc401093",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/nethan/Documents/University/CSE_487_2/487_Phase2/depression_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m script_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\n\u001b[1;32m     20\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(script_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepression_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(dataset_path)\n\u001b[1;32m     23\u001b[0m data\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/nethan/Documents/University/CSE_487_2/487_Phase2/depression_data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "script_dir = os.getcwd()\n",
    "dataset_path = os.path.join(script_dir, 'depression_data.csv')\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966f7fc5-0c80-4647-9df9-70dc96ac34f1",
   "metadata": {},
   "source": [
    "**EDA Steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce81b8-450b-453f-9b5e-dc9c6072b4ae",
   "metadata": {},
   "source": [
    "**Feature engineering, Dropping the 'Name' column to protect users privacy and because it is an unneccesary feature that is causally\n",
    "impotent and cannot effect our response variable.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60782f2-fdae-4b0a-9d41-e1f95e4fa7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['Name'], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4819a93-9db0-4b09-b453-01b521a78314",
   "metadata": {},
   "source": [
    "**Displaying general characteristics of the quantititative features: 'Age', 'Number of Children' and 'Income'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a9e2ce-9307-4f2c-8982-6ff2ab50f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryAge = data['Age'].describe()\n",
    "print(summaryAge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19d7dd6-6900-4363-97ca-a71ccab4533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "plt.hist(data['Age'], bins=6, edgecolor='black')\n",
    "plt.title('Histogram of Age')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot (outliers)\n",
    "sns.boxplot(x=data['Age'])\n",
    "plt.title('Boxplot of Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af43f389-5b1b-412e-9df8-f050c33aadce",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryNumChildren = data['Number of Children'].describe()\n",
    "print(summaryNumChildren)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eee1861-b1fa-4cf8-ab60-303f93df58a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "plt.hist(data['Number of Children'], bins=4, edgecolor='black')\n",
    "plt.title('Histogram of Number of Children')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot (outliers)\n",
    "sns.boxplot(x=data['Number of Children'])\n",
    "plt.title('Boxplot of Number of Children')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e5f2c0-bbb6-415d-8f64-d02fcd23d31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryIncome = data['Income'].describe()\n",
    "print(summaryIncome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56d81cd-5399-4f6e-b3e1-083fdbb3e77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "plt.hist(data['Income'], bins=10, edgecolor='black')\n",
    "plt.title('Histogram of Income')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot (outliers)\n",
    "sns.boxplot(x=data['Income'])\n",
    "plt.title('Boxplot of Income')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d072266d-5aa4-4a62-9476-a1c02c1db8b9",
   "metadata": {},
   "source": [
    "**Investigating and visualizing relationship between categorical features and and depression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdff98f-3633-45eb-a0bb-d86d47266f9d",
   "metadata": {},
   "source": [
    "**A) Investigating the breakdown of 'Marital Status' amongst the depressed sample population**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84639131-ddfe-41bf-b798-71cff1ade42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "maritalStatusCounts = data['Marital Status'].value_counts()\n",
    "maritalStatusCounts.plot(kind='bar')\n",
    "plt.xlabel('Marital Status')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Breakdown of Marital Status Amongst the Depressed Sample Population')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f45da89-a364-4691-94a6-5e3ae3eba13f",
   "metadata": {},
   "source": [
    "The bar graph clearly shows that most of the depressed individuals are married, this suggests an increased association between being married and being depressed. This sets us up nicely for a linear regression analysis down the line to predict how being married can impact depression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bffada-3547-462a-b434-58d91098444c",
   "metadata": {},
   "source": [
    "Interestingly, divorced individuals have the lowest frequency amongst our depressed sample population, this might suggest that perhaps being unburdened by a large stress inducing factor (unhealthy relationships) can be associated with positive mental health. We could further investigate the hypothesis that being unburdened by a large stress inducing factor, be it an unhealthy relationship or not worrying about finances, can lead to better mental health through methods such as k-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64836c2c-99f3-4592-a521-0ec4766a37e9",
   "metadata": {},
   "source": [
    "**B) Investigating the relationship between having 'Chronic Medical Conditions' and depression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3456917-0fa7-4739-8428-ca3e4d17cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "medCount = data['Chronic Medical Conditions'].value_counts()\n",
    "medCount.plot(kind='bar')\n",
    "plt.xlabel('Invidual has Chronic Medical Conditions ?')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Breakdown of Depressed Individuals that have Chronic Medical Conditions')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6414a20a-2b07-4535-940a-58eaec24c41b",
   "metadata": {},
   "source": [
    "This graph is interesting becuase 'Chronic Medical Conditions' could be causally affected by other lifestlye features in the dataset like 'Smoking Status', 'Physical Activity Level', 'Dietary Habits' and 'Alcohol Consumption', therefore if there was a high frequency of Chronically Ill individuals who are depressed, it could suggest an association between depression and being chronically ill. This graph does not show that, it instead suggests that being chronically ill is not strongly associated with depression, implying that given limited resources and time, investigating the relationship between these lifestlye factors and their effect on health and in turn the potential to reduce probability of developing depression might not yield a significant result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396d43c3-bb9e-40f4-97f8-81b7855deea4",
   "metadata": {},
   "source": [
    "**Multivariate Analysis :**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c946dd80-8d1c-4f5a-9f54-c9abdfc92c1f",
   "metadata": {},
   "source": [
    "**A) Generating a heatmap to investigate correlation between the features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22315d-1fd3-4dea-9320-67285b92e266",
   "metadata": {},
   "source": [
    "Data PreProcessing: Encoding all the ordinal categorical variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a417d8-1239-4973-8b35-d55e21bb5541",
   "metadata": {},
   "source": [
    "Mapping and encoding unique education levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d27c4-75ff-431c-99e2-34dfb39708ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "eduLevels = data[\"Education Level\"].unique()\n",
    "print(eduLevels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c570732-549c-41ea-bb7c-22990e0c4e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "eduMap = {\n",
    "    'High School': 0,\n",
    "    'Associate Degree': 1,\n",
    "    \"Bachelor's Degree\": 2,\n",
    "    \"Master's Degree\": 3,\n",
    "    'PhD': 4\n",
    "}\n",
    "data[\"Education Level\"] = data[\"Education Level\"].map(eduMap)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f70191-3922-41cf-8b03-789b0f0e58d3",
   "metadata": {},
   "source": [
    "**Mapping and Encoding Activity Levels:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a525f5f-cdef-43a1-a089-ffee2c1bdad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "actLevels = data[\"Physical Activity Level\"].unique()\n",
    "print(actLevels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6d5772-c793-45ba-8909-36c5e48fa8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "actMap = {\n",
    "    'Sedentary': 0,\n",
    "    'Moderate': 1,\n",
    "    'Active': 2\n",
    "}\n",
    "\n",
    "data[\"Physical Activity Level\"] = data[\"Physical Activity Level\"].map(actMap)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ae55a3-d959-4343-ac07-fd6135faf21d",
   "metadata": {},
   "source": [
    "**Mapping and Encoding Alcohol Consmuption, Dietary Habits and Sleep Patterns:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02597903-7226-4248-99a4-aa773d659906",
   "metadata": {},
   "outputs": [],
   "source": [
    "sleepLevels = data[\"Sleep Patterns\"].unique()\n",
    "alcLevels = data[\"Alcohol Consumption\"].unique()\n",
    "dietLevels = data[\"Dietary Habits\"].unique()\n",
    "\n",
    "print(sleepLevels)\n",
    "print(alcLevels)\n",
    "print(dietLevels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090cd5e9-047f-43c2-8760-f137708da669",
   "metadata": {},
   "outputs": [],
   "source": [
    "sleepMap = {\n",
    "    'Fair': 1,\n",
    "    'Good': 2,\n",
    "    'Poor': 0\n",
    "}\n",
    "\n",
    "alcMap = {\n",
    "    'Moderate':1,\n",
    "    'High':2,\n",
    "    'Low': 0\n",
    "}\n",
    "\n",
    "dietMap = {\n",
    "    'Moderate':1,\n",
    "    'Unhealthy': 0,\n",
    "    'Healthy': 2\n",
    "}\n",
    "\n",
    "data['Sleep Patterns'] = data['Sleep Patterns'].map(sleepMap)\n",
    "data['Alcohol Consumption'] = data['Alcohol Consumption'].map(alcMap)\n",
    "data['Dietary Habits'] = data['Dietary Habits'].map(dietMap)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d537c428-5e0c-4604-9f8e-972dcd11cab7",
   "metadata": {},
   "source": [
    "**Encoding of smoking status:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c9b506-b234-4f5c-ba53-04ba317bfc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "smokingLevels = data['Smoking Status'].unique()\n",
    "print(smokingLevels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84357649-21f1-4ffc-98d5-d25ca78942cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "smokeMap = {\n",
    "    'Non-smoker': 2,\n",
    "    'Former': 1,\n",
    "    'Current': 0\n",
    "}\n",
    "data['Smoking Status'] = data['Smoking Status'].map(smokeMap)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075f6868-fd42-440f-89f7-967e615f65f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "yesMap = {\n",
    "    'Yes': 1,\n",
    "    'No': 0\n",
    "}\n",
    "\n",
    "data['History of Mental Illness'] = data['History of Mental Illness'].map(yesMap)\n",
    "data['History of Substance Abuse'] = data['History of Substance Abuse'].map(yesMap)\n",
    "data['Family History of Depression'] = data['Family History of Depression'].map(yesMap)\n",
    "data['Chronic Medical Conditions'] = data['Chronic Medical Conditions'].map(yesMap)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e385f4-3798-4b0b-8d56-101cb4d37fea",
   "metadata": {},
   "source": [
    "**Employment Encoding:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07e2df6-36ed-488c-add1-1e091c56f6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "empLevels = data['Employment Status'].unique()\n",
    "print(empLevels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1728ee8b-656d-48c4-984c-1c4213597304",
   "metadata": {},
   "outputs": [],
   "source": [
    "empMap = {\n",
    "    'Unemployed':0,\n",
    "    'Employed': 1\n",
    "}\n",
    "\n",
    "data['Employment Status'] = data['Employment Status'].map(empMap)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efe8112-25a7-4e76-9f79-0bf1fbf5cb09",
   "metadata": {},
   "source": [
    "**Correlation Heatmap of all features except marital status (marital status is neither ordinal or binary so it cannot be encoded without imbueing bias):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb0c93c-74be-4a44-825f-3a74f464db2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corData = data.drop('Marital Status', axis=1)\n",
    "\n",
    "corrMat = corData.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corrMat, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "\n",
    "# Display the heatmap\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb1d0da-f277-4e17-97c9-5040af9b7127",
   "metadata": {},
   "source": [
    "The correlation heatmap uncovers alot of interesting relationships between the variables. \n",
    "\n",
    "As expected, there is a relatively strong positive correlation between Education Level and Income, this suggests that employment status might be a good predictor of income. Digging deeper into the association of income with depression would allow us to explore whether pursuing higher education would reduce the chances of developing depression because it increases the potential income you could earn. \n",
    "\n",
    "Another interesting insight you can glean is the relatively strong negative correlation between Number of Children and Sleep patterns, this suggests that an increasing number of children could predict worse sleeping habits, analysing the relationship between bad sleep and depression could answer questions like \"Could having more children lead to an increased chance of developing depression\"\n",
    "\n",
    "A relatively strong negative correlation between Age with employment status and Age with Physical Activity Level could suggest that the older you get, there might be an increased chances of developing depression becuase there is a higher chance of being less active and employed. Exploring the relationship between activity level and Income would help us answer this question. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256ec49f-c285-442f-aa28-199fb66e837b",
   "metadata": {},
   "source": [
    "Investigating the relationship between sleep patterns and number of children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff64a66d-9bc7-4ebb-94c6-911953591d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_labels = {0: \"Poor\", 1: \"Fair\", 2: \"Good\"}\n",
    "data[\"Sleep Patterns\"] = data[\"Sleep Patterns\"].map(sleep_labels)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(data=data, x=\"Sleep Patterns\", y=\"Number of Children\", palette=\"coolwarm\")\n",
    "plt.xlabel(\"Sleep Pattern\")\n",
    "plt.ylabel(\"Average number of Children\")\n",
    "plt.title(\"Number of Children and Sleep Pattern\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be555bad-ac39-4237-94e3-9a27996fc677",
   "metadata": {},
   "source": [
    "The bar graph is to observe the relationship between the pattern of sleep and the number of children. This is to investigate factors that associate with depression which one of them being sleep. We wanted to observe how different variables affect sleep. In this case we decided to use the number of children."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41d1e01-cbda-4d04-8ab1-35e96ca5b7dc",
   "metadata": {},
   "source": [
    "**PROJECT PHASE 2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3517f1-6461-421e-9fc9-1f43943266d6",
   "metadata": {},
   "source": [
    "**Linear Regression to predict if pursuing a higher education can result in a higher income**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97f9e1c-05d4-4a2c-b9ee-3472ef970127",
   "metadata": {},
   "source": [
    "Split the data into a training set and a test set, 90% for training and 10% for testing purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9dad17-e774-4bad-8f71-a925eb42db9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[[\"Education Level\"]]  \n",
    "y = data[\"Income\"]  \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=83)\n",
    "\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bf757d-7f00-464b-876a-ee4bdcb68a26",
   "metadata": {},
   "source": [
    "Training the model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7494d33e-133e-4a04-8227-39cddfdad579",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(f\"Intercept: {model.intercept_}\")\n",
    "print(f\"Coefficient for Education Level: {model.coef_[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc170a-1f8c-4c67-8305-ba7cdb2eef3f",
   "metadata": {},
   "source": [
    "Predicting Income from test Education level data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a6e6e3-8cc8-4c24-8da1-0a192bbae912",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "results = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred})\n",
    "print(results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3c1290-8149-4d5b-a508-b169abf6e3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7087533-8022-44b8-975e-b859c6f2da4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=y_test, y=y_pred)\n",
    "plt.xlabel(\"Actual Income\")\n",
    "plt.ylabel(\"Predicted Income\")\n",
    "plt.title(\"Actual vs. Predicted Income\")\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color=\"red\", linestyle=\"--\")  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e93ec1-bd4e-4367-8410-aa3b9de7634c",
   "metadata": {},
   "source": [
    "**Justification for Choosing Linear Regression:**\n",
    "We selected Linear Regression for this task to predict whether pursuing a higher education leads to a higher income, as it provides a straightforward method for modeling the relationship between a continuous dependent variable (Income) and an independent variable (Education Level). Linear Regression is commonly used when the goal is to understand how changes in an independent variable affect a dependent variable, and it is particularly effective when there is a linear relationship between the two as the correlation heatmap suggested. \n",
    "\n",
    "**Work Done to Train and Tune the Model:**\n",
    "\n",
    "We started by selecting the relevant features for the model. The independent variable (X) was the Education Level, and the dependent variable (y) was Income.\n",
    "The data was then split into a training set (90% of the data) and a test set (10% of the data) using train_test_split to ensure that the model is trained on a large portion of the data while still being evaluated on a separate subset.\n",
    "\n",
    "**Analysis of Results:**\n",
    "Accuracy of Predictions: The R² score of 0.2839 shows that the model has limited explanatory power.\n",
    "\n",
    "Error Metrics: The MAE and RMSE values suggest that the model's predictions are somewhat off, with a typical prediction error of around $26,463 (MAE) and $34,556 (RMSE). it highlights that the model is not highly precise, especially when trying to predict exact income values.\n",
    "\n",
    "While it does capture the overall trend that a higher education level results in higher income, the poor predictive power might be down to the fact that the Independent variable was categorical and was encoded into discrete values. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcda250-6915-42db-89e0-5e3a0fd66423",
   "metadata": {},
   "source": [
    "**Implementing K Means**\n",
    "\n",
    "scaling data to be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9146f915-b98a-4d93-b662-910e392d1a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledData = corData.copy()\n",
    "for col in scaledData.columns:\n",
    "    avg = scaledData[col].mean()\n",
    "    sd = scaledData[col].std()\n",
    "    scaledData[col] = scaledData[col].apply(lambda x: (x - avg)/sd)\n",
    "scaledData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345d163e-6a51-49c6-a77b-8b41b6220b57",
   "metadata": {},
   "source": [
    "**Using Elbow Method to Determine the Optimal Number of Clusters 'k'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08e7a92-8d44-46ca-b56f-749d6c833c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = []\n",
    "for numK in range (1, 40):\n",
    "    km = KMeans(n_clusters = numK)\n",
    "    km.fit(scaledData)\n",
    "    dist.append([numK, km.inertia_])\n",
    "dist = pd.DataFrame(dist, columns=['numK', 'distance'])\n",
    "\n",
    "dist.set_index('numK').plot()\n",
    "plt.xlabel('numk')\n",
    "plt.ylabel('dist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a67f026-e5e9-47d1-aed1-930b8621aa1b",
   "metadata": {},
   "source": [
    "**Judging from the graph, the curve flattens when k is ~ 11.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33435c27-1251-40b4-84c4-cff41b353296",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=11)\n",
    "scaledData['cluster'] = km.fit_predict(scaledData)\n",
    "\n",
    "scaledData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56003a57-70de-453d-999b-78627e5cfcef",
   "metadata": {},
   "source": [
    "**Breakdown of clusters and their associated data values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ccd43a-9f8b-4e70-bdf8-1bbfe14ac04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts = scaledData['cluster'].value_counts()\n",
    "plt.figure(figsize=(10, 6))\n",
    "cluster_counts.sort_index().plot(kind='bar', color='skyblue')\n",
    "plt.title('Cluster Distribution')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e302d29c-d549-4913-87b2-b466a398c056",
   "metadata": {},
   "source": [
    "**Justification for Choosing K-Means:**\n",
    "We selected the K-Means algorithm for this clustering task due to its simplicity, efficiency, and effectiveness in partitioning data into well-defined groups. As an unsupervised learning model, K-Means allowed us to explore the dataset without predefined labels, helping us identify any underlying patterns, clusters, or similarities within the data.\n",
    "\n",
    "**Work Done to Tune/Train the Model:**\n",
    "To prepare the data for clustering, we first scaled the features using z-score normalization. This step was necessary to ensure that all variables contribute equally to the clustering process, as K-Means is sensitive to the scale of the data. Each feature was centered around its mean and scaled by its standard deviation to achieve standardization.\n",
    "\n",
    "We then used the Elbow Method to determine the optimal number of clusters, k. We iterated through different values of k (from 1 to 39) and calculated the inertia (sum of squared distances from each point to its assigned cluster center). The inertia was plotted against the number of clusters, and the \"elbow\" in the graph occurred around k=11, where the rate of decrease in inertia slowed significantly. This suggested that k=11 was the most suitable choice for the number of clusters.\n",
    "\n",
    "**Insights from the Algorithm:**\n",
    "The K-Means algorithm effectively identified 11 distinct clusters within the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd74f75-705c-455f-9c19-02adcf58d5df",
   "metadata": {},
   "source": [
    "**Using K-nn to Predict If an Individual has Chronic Medical Conditions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c3f4b4-8aca-4e99-b38f-d87b238e4e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['Chronic Medical Conditions'].value_counts())\n",
    "corData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f41190-ae5b-4d92-a53d-e0d04e16ac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "knnScaled = scaledData.drop('cluster', axis=1)\n",
    "knnScaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6305d5bc-5ea9-46ef-8bf9-b3f65013e8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = knnScaled.drop('Chronic Medical Conditions', axis=1) \n",
    "y = corData['Chronic Medical Conditions']   \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=99)\n",
    "print(y_train.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828fc466-6286-4594-9d9e-e643244cf750",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ae3f2d-fc8f-4a1a-bbd4-003cad2cc2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b80f8b-e270-4b47-b3b8-b57a8ca32979",
   "metadata": {},
   "source": [
    "Justification for Choosing K-Nearest Neighbors (KNN):\n",
    "We selected K-Nearest Neighbors (KNN) for this binary classification task due to its simplicity, effectiveness in being used as a binary classifier like in our context where invidual either has Chronic Medical Conditions (1) or not (0).\n",
    "\n",
    "When K=5 the model performs decently well with a 61% accuracy however, The significant class imbalance (2:1) is impacting the model, especially in predicting class 1 (chronic medical conditions). While recall for class 0 is good, recall for class 1 is very low, leading to high false negatives. The low recall and F1-score for class 1 indicate that the model struggles to identify instances of chronic medical conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e728a59-2c19-4bfa-aeb5-bd197e95a71a",
   "metadata": {},
   "source": [
    "**Using Cross validation to find a better value for 'K' to improve model perfomance**\n",
    "\n",
    "Due to PC resource and time constraints, we will try and find the most optimal number of neighbours from 1 to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a39adf-7e4c-4d96-afca-e9939023400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "k_range = range(1, 11)  \n",
    "mean_accuracies = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X, y, cv=3, scoring='accuracy', n_jobs=-1)  \n",
    "    mean_accuracies.append(scores.mean())  \n",
    "\n",
    "plt.plot(k_range, mean_accuracies, marker='o')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Cross-Validated Accuracy')\n",
    "plt.title('Cross-Validation: Finding the Optimal k for KNN')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100533dc-1cd3-4a68-ab83-b2b0e023aeaa",
   "metadata": {},
   "source": [
    "The graph shows that the best accuracy is acheived using K=10. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bb0514-25f5-4a32-86fd-e6cbc219ba14",
   "metadata": {},
   "source": [
    "**Re Implementation of Knn on K=10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d155691a-7874-4e19-9b02-437b2874f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f1c20-29a4-4d69-86d0-bdd1d622f1ee",
   "metadata": {},
   "source": [
    "**Work Done to Tune/Train the Model:**\n",
    "Initially, K=5 was used, but the performance did not meet expectations due to the significant class imbalance. Therefore, cross-validation was employed to find the optimal number of neighbors, with a search range from 1 to 10. The value K=10 was selected as it provided the best accuracy, improving the model's performance relative to K=5. However, the class imbalance remains a challenge, which led to less-than-optimal performance for the minority class (class 1).\n",
    "\n",
    "**Effectiveness of the Algorithm:**\n",
    "Precision: The precision for class 0 remained decent at 0.68, which means that when the model predicted class 0, it was correct 68% of the time. However, the precision for class 1 remained low at 0.36, indicating a higher false-positive rate when predicting class 1.\n",
    "\n",
    "**Recall:** The recall for class 0 was high at 0.92, suggesting that the model successfully identified most of the instances of class 0. However, recall for class 1 dropped significantly to 0.09, indicating that the model struggles to identify class 1 instances, leading to a large number of false negatives.\n",
    "\n",
    "**F1-Score:** The F1-score for class 1 is 0.14, which is very low, further emphasizing the model's inability to effectively identify instances of class 1 (chronic medical conditions). In contrast, class 0 had a better F1-score of 0.78, which reflects the model's stronger performance on the majority class.\n",
    "\n",
    "**Accuracy:** The accuracy of 65.1% is an improvement over previous configurations (such as with K=5), but it still falls short due to the imbalanced nature of the dataset. While the model is highly accurate at predicting class 0, its performance on class 1 is poor due to the low recall and F1-score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2d37da-b576-4d40-b366-755eb9a8f5ca",
   "metadata": {},
   "source": [
    "**Implementing Logistic to predict if having a higher income was associated with being employed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2fbd5-7614-4a6d-a834-99e27af58021",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = corData[['Income']].values\n",
    "Y = corData['Employment Status'].values \n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=82)\n",
    "model = LogisticRegression()  \n",
    "model.fit(X_train, Y_train)  \n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x= X_train.flatten(), y= Y_train, alpha=0.7, color='blue')\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Employment Status (0 = Unemployed, 1 = Employed)')\n",
    "plt.title('Relationship Between Employment Status and Income with trained model')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "conf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "class_report = classification_report(Y_test, Y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc25d503-7413-4ea4-8d4c-ff7a1ef78041",
   "metadata": {},
   "source": [
    "**Justificaton for choosing logisitic regression:** Logisitic regression was used because Employment Status was converted to a binary outcome during data cleaning in phase 1 (0 = Unemployed, 1 = Employed) and income is continous making it ideal to predict binary outcome to see if an individual is employeed or not.\n",
    "\n",
    "**Work Done to Tune/Train the Model:** We started by selecting the relevant features for the model. The independent variable (X) was the Income, and the dependent variable (y) was Employment status. The data was then split into a training set (80% of the data) and a test set (20% of the data) using train_test_split to ensure that the model is trained on a large portion of the data while still being evaluated on a separate subset\n",
    "\n",
    "**Effectiveness of the Algorithm:** Our model has a 93% accuracy with high precision (Unemployed = 89%, Employed = 95%) and recall (Unemployed = 90%, Employed = 94%) meaning it predicts employment status well with minimal bias. The F1-scores (Unemployed: 90%, Employed: 94%) confirm a strong balance between precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b17931-f61c-404d-a50f-f01c407e89b4",
   "metadata": {},
   "source": [
    "**Implementing Support Vector Machine Algorithm to predict if an Individual Has a History of Mental Illness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5d995-d109-448d-9db7-4397c39a0f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svmFrame = scaledData.drop('cluster', axis=1)\n",
    "X = svmFrame.drop('History of Mental Illness', axis=1) \n",
    "y = corData['History of Mental Illness'] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=109)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a935ba2-0a82-459f-9a22-9f19432d767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LinearSVC()\n",
    "model = LinearSVC(class_weight=\"balanced\", max_iter=5000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54431b27-52d2-49b1-ac43-3e208ea04594",
   "metadata": {},
   "source": [
    "**Justification for Choosing Linear SVM:**\n",
    "We selected the Linear SVM (specifically LinearSVC) for this classification task due to its efficiency and optimization for large datasets, making it suitable for our context where computational and time constraints were key factors. LinearSVC is faster compared to other SVM variants, especially when dealing with high-dimensional data like ours.\n",
    "\n",
    "**Work Done to Tune/Train the Model:**\n",
    "To address the class imbalance in our dataset, where class 0 (no history of mental illness) was highly dominant over class 1 (with a history of mental illness), we used the class_weight=\"balanced\" parameter. This automatically adjusts the weights inversely proportional to the class frequencies in the input data, making the algorithm more sensitive to the minority class. Additionally, we set max_iter=5000 to ensure the model converged effectively within the limits imposed by the data size and the number of features.\n",
    "\n",
    "**Effectiveness of the Algorithm:**\n",
    "Precision: The model achieved a precision of 0.75 for class 0, which indicates that when it predicted class 0, it was correct 75% of the time. However, the precision for class 1 was very low at 0.38, showing that many of the predicted instances of class 1 were incorrect.\n",
    "\n",
    "**Recall:** The recall for class 1 (0.50) was higher than precision, meaning the model was able to identify some of the true instances of class 1 but still missed a considerable portion. Class 0 had a recall of 0.65, indicating that it was reasonably good at identifying most of the instances of the dominant class.\n",
    "\n",
    "**F1-Score:** The F1-score, which is the harmonic mean of precision and recall, was low for both classes, especially for class 1, indicating an overall suboptimal balance between precision and recall. The weighted F1-score of 0.61 suggests the model performed better in identifying class 0 but was still far from ideal.\n",
    "\n",
    "**Accuracy:** The overall accuracy of 60% isn't great, mainly because of the imbalance in the dataset, where class 1 was underrepresented, and the model had a tendency to favor the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae7316e-4bc7-4dee-a588-2b5627e5ab8f",
   "metadata": {},
   "source": [
    "**Decision Tree Analysis on Dietary Habits based on Education Level and Physical Activity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674a3228-1599-4add-b052-477fa2b2a618",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Education Level', 'Physical Activity Level']\n",
    "X = corData[columns]\n",
    "Y = corData['Dietary Habits']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "dtree = DecisionTreeClassifier(criterion='entropy', max_depth=10, random_state=42)\n",
    "dtree.fit(X_train, Y_train)\n",
    "\n",
    "#We decided to not plot the tree because it looks too messy. but feel free to uncomment to \n",
    "# plt.figure(figsize=(20,16))\n",
    "# plot_tree(dtree, feature_names=columns, class_names=['Unhealthy', 'Moderate', 'Healthy'], filled=True, rounded=True)\n",
    "# plt.title('Decision Tree for Dietary Habits based of Education Level and Physical Activity')\n",
    "# plt.show()\n",
    "\n",
    "importance = pd.DataFrame({'column': columns, 'Importance': dtree.feature_importances_})\n",
    "importance = importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=importance['Importance'], y=importance['column'], palette='coolwarm')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Importance in Determining Dietary Habits')\n",
    "plt.show()\n",
    "\n",
    "Y_pred = dtree.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aab86a-1429-4d06-8eb4-729c6cd09b72",
   "metadata": {},
   "source": [
    "**Justificaton for choosing Decision Tree** Decision Tree was used because it would help us classify which factors (Education Level and Physical Activity Level) was most likely to affect Dietary habits.\n",
    "\n",
    "**Source:** https://www.geeksforgeeks.org/decision-tree/\n",
    "\n",
    "**Work Done to Tune/Train the Model:** The independent variables (X) was Education Level and Physical Activity Levels while the dependent variable (y) was Dietary Habits. The data was then split into a training set (80% of the data) and a test set (20% of the data) using train_test_split to ensure that the model is trained on a large portion of the data while still being evaluated on a separate subset\n",
    "\n",
    "**Effectiveness of the Algorithm:** Our model had suboptimal performance making our accuracy 55% probably due to the lack of features we used to capture the complexity of dietary habits. The reason for the limited amount of features was because we couldn't find other highly correlated features in the heat map we computed in phase 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b3ac08-66a8-4392-b122-4fbdf78b930f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
